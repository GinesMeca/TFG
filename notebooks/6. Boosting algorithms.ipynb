{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Boosting algortihms\n",
    "\n",
    "This notebook will show applications of boosting algorithms on the dataset [Rain in Australia](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) available in Kaggle. It contains about 10 years of daily weather observations from many locations across Australia.\n",
    "\n",
    "### Index:\n",
    "1. [Packages required](#1.-Packages-required)\n",
    "2. [Loading data](#2.-Loading-data)\n",
    "3. [AdaBoost](#3.-AdaBoost)\n",
    "4. [Gradient Boosting](#4.-Gradient-Boosting)\n",
    "5. [XGBoost](#5.-XGBoost)\n",
    "6. [LightGBM](#6.-LightGBM)\n",
    "7. [CatBoost](#7.-CatBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install lightgbm\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>Date_month</th>\n",
       "      <th>Date_day</th>\n",
       "      <th>Location_encoded</th>\n",
       "      <th>WindGustDir_encoded</th>\n",
       "      <th>WindDir9am_encoded</th>\n",
       "      <th>WindDir3pm_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0 2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1 2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2 2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3 2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4 2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Temp9am  Temp3pm  RainToday  \\\n",
       "0           W           44.0          W  ...    16.9     21.8          0   \n",
       "1         WNW           44.0        NNW  ...    17.2     24.3          0   \n",
       "2         WSW           46.0          W  ...    21.0     23.2          0   \n",
       "3          NE           24.0         SE  ...    18.1     26.5          0   \n",
       "4           W           41.0        ENE  ...    17.8     29.7          0   \n",
       "\n",
       "   RainTomorrow  Date_month  Date_day  Location_encoded  WindGustDir_encoded  \\\n",
       "0           0.0          12         1                 2                 12.0   \n",
       "1           0.0          12         2                 2                 13.0   \n",
       "2           0.0          12         3                 2                 11.0   \n",
       "3           0.0          12         4                 2                  2.0   \n",
       "4           0.0          12         5                 2                 12.0   \n",
       "\n",
       "   WindDir9am_encoded  WindDir3pm_encoded  \n",
       "0                12.0                13.0  \n",
       "1                15.0                11.0  \n",
       "2                12.0                11.0  \n",
       "3                 6.0                 4.0  \n",
       "4                 3.0                14.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_parquet('../data/04_model_input/master.parquet')\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fix the variables we are interested in and the date to separate data:\n",
    "test_date = '2015-01-01'\n",
    "\n",
    "model_columns = list(set(weather.select_dtypes(include='number').columns) - set(['RainTomorrow']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We separate in train/test data and solve Nan problems:\n",
    "train = weather[weather.Date < test_date].fillna(-1)\n",
    "test = weather[weather.Date >= test_date].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost is a boosting algorithm that reduces the prediction error building (sequentially) trees with only two leave nodes. According to the error from the last estimator, the sample weights are changed and the trees are generated taking into account these different weights.\n",
    "\n",
    "The Python implementation allows you to modify the base estimator, but we won't modify it to use the original one (\"base_estimator = None\" is a decision tree with max_depth = 1). Also, to avoid overfitting, we will use a learning rate $\\nu = 0.1$\n",
    "\n",
    "Then, we will generate different model with different number of boosting iterations to see the evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_Gini</th>\n",
       "      <th>Test_Gini</th>\n",
       "      <th>delta%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaB_1</th>\n",
       "      <td>0.356810</td>\n",
       "      <td>0.324403</td>\n",
       "      <td>-9.082368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_5</th>\n",
       "      <td>0.540856</td>\n",
       "      <td>0.518804</td>\n",
       "      <td>-4.077312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_10</th>\n",
       "      <td>0.569703</td>\n",
       "      <td>0.540461</td>\n",
       "      <td>-5.132848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_20</th>\n",
       "      <td>0.644189</td>\n",
       "      <td>0.607569</td>\n",
       "      <td>-5.684610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_50</th>\n",
       "      <td>0.690590</td>\n",
       "      <td>0.660897</td>\n",
       "      <td>-4.299631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_100</th>\n",
       "      <td>0.709561</td>\n",
       "      <td>0.690381</td>\n",
       "      <td>-2.703051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_200</th>\n",
       "      <td>0.725582</td>\n",
       "      <td>0.711206</td>\n",
       "      <td>-1.981235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_500</th>\n",
       "      <td>0.738811</td>\n",
       "      <td>0.723546</td>\n",
       "      <td>-2.066107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_1000</th>\n",
       "      <td>0.745362</td>\n",
       "      <td>0.727811</td>\n",
       "      <td>-2.354736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Train_Gini  Test_Gini    delta%\n",
       "AdaB_1       0.356810   0.324403 -9.082368\n",
       "AdaB_5       0.540856   0.518804 -4.077312\n",
       "AdaB_10      0.569703   0.540461 -5.132848\n",
       "AdaB_20      0.644189   0.607569 -5.684610\n",
       "AdaB_50      0.690590   0.660897 -4.299631\n",
       "AdaB_100     0.709561   0.690381 -2.703051\n",
       "AdaB_200     0.725582   0.711206 -1.981235\n",
       "AdaB_500     0.738811   0.723546 -2.066107\n",
       "AdaB_1000    0.745362   0.727811 -2.354736"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We generate our AdaBoost algorithms:\n",
    "metrics = {}\n",
    "for n_estimators in [1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    model = AdaBoostClassifier(n_estimators = n_estimators, learning_rate = 0.1)\n",
    "    model.fit(train[model_columns],train.RainTomorrow);\n",
    "    \n",
    "    train_pred = model.predict_proba(train[model_columns])[:, 1]\n",
    "    test_pred = model.predict_proba(test[model_columns])[:, 1]\n",
    "\n",
    "    metrics['AdaB_'+ str(n_estimators)] = {\n",
    "        'Train_Gini': 2*roc_auc_score(train.RainTomorrow, train_pred)-1,\n",
    "        'Test_Gini': 2*roc_auc_score(test.RainTomorrow, test_pred)-1\n",
    "    }\n",
    "\n",
    "metrics_AdaB = pd.DataFrame.from_dict(metrics, orient='index',columns=['Train_Gini', 'Test_Gini'])\n",
    "metrics_AdaB['delta%'] = 100*(metrics_AdaB.Test_Gini - metrics_AdaB.Train_Gini) / metrics_AdaB.Train_Gini\n",
    "metrics_AdaB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is another boosting algorithm. While AdaBoost modifies the sample weights to build the trees, Gradient Boosting computes the residuals and try to classify them. By this way, the model starts with a big error but the more iterations you make the less error you will be comitting. Also, other difference between AdaBoost and Gradient Boosting is the fact that AdaBoost (originally) builds trees with only two leave nodes and Gradient Boost doesn't have a predetermined number of leaves.\n",
    "\n",
    "About the parameters that we will choose to build our model:\n",
    "* Loss function: loss = log_loss (default), that it's the same that we have been studying at the project.\n",
    "* Learning rate: learning_rate = 0.1 (default), that it's the most common value to avoid overfitting.\n",
    "* Error measure: criterion = mse, that it's the same that we have been studying.\n",
    "\n",
    "Also, we will generate models with different numbers of iterations (n_estimators) to compare them and see clearly the evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_Gini</th>\n",
       "      <th>Test_Gini</th>\n",
       "      <th>delta%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GB_1</th>\n",
       "      <td>0.576891</td>\n",
       "      <td>0.564835</td>\n",
       "      <td>-2.089953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_5</th>\n",
       "      <td>0.652841</td>\n",
       "      <td>0.632492</td>\n",
       "      <td>-3.117064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_10</th>\n",
       "      <td>0.673436</td>\n",
       "      <td>0.653937</td>\n",
       "      <td>-2.895338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_20</th>\n",
       "      <td>0.702074</td>\n",
       "      <td>0.685878</td>\n",
       "      <td>-2.306969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_50</th>\n",
       "      <td>0.736219</td>\n",
       "      <td>0.718145</td>\n",
       "      <td>-2.454960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_100</th>\n",
       "      <td>0.756702</td>\n",
       "      <td>0.735576</td>\n",
       "      <td>-2.791743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_200</th>\n",
       "      <td>0.775952</td>\n",
       "      <td>0.749864</td>\n",
       "      <td>-3.362149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_500</th>\n",
       "      <td>0.803554</td>\n",
       "      <td>0.766653</td>\n",
       "      <td>-4.592306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_1000</th>\n",
       "      <td>0.825757</td>\n",
       "      <td>0.771226</td>\n",
       "      <td>-6.603767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Train_Gini  Test_Gini    delta%\n",
       "GB_1       0.576891   0.564835 -2.089953\n",
       "GB_5       0.652841   0.632492 -3.117064\n",
       "GB_10      0.673436   0.653937 -2.895338\n",
       "GB_20      0.702074   0.685878 -2.306969\n",
       "GB_50      0.736219   0.718145 -2.454960\n",
       "GB_100     0.756702   0.735576 -2.791743\n",
       "GB_200     0.775952   0.749864 -3.362149\n",
       "GB_500     0.803554   0.766653 -4.592306\n",
       "GB_1000    0.825757   0.771226 -6.603767"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We generate our Gradient Boosting algorithms:\n",
    "metrics = {}\n",
    "for n_estimators in [1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    model = GradientBoostingClassifier(n_estimators = n_estimators, criterion = 'mse' )\n",
    "    model.fit(train[model_columns],train.RainTomorrow);\n",
    "    \n",
    "    train_pred = model.predict_proba(train[model_columns])[:, 1]\n",
    "    test_pred = model.predict_proba(test[model_columns])[:, 1]\n",
    "\n",
    "    metrics['GB_'+ str(n_estimators)] = {\n",
    "        'Train_Gini': 2*roc_auc_score(train.RainTomorrow, train_pred)-1,\n",
    "        'Test_Gini': 2*roc_auc_score(test.RainTomorrow, test_pred)-1\n",
    "    }\n",
    "\n",
    "metrics_GB = pd.DataFrame.from_dict(metrics, orient='index',columns=['Train_Gini', 'Test_Gini'])\n",
    "metrics_GB['delta%'] = 100*(metrics_GB.Test_Gini - metrics_GB.Train_Gini) / metrics_GB.Train_Gini\n",
    "metrics_GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is the abbreviation of 'e**X**treme **G**radient **Boost**ing' and is a boosting method based on the last one. XGBoost models apply Gradient Boosting using their own type of trees, which are built taking into account the gradient and the hessian of the Loss Function and some regularization parameters. Also, XGBoost has many computational advantages to do the task faster.\n",
    "\n",
    "XGBoost is a algorithm with a lot of parameters: regularization parameters, maximum depth of the trees, number of iterations, minimum number of subjects in the node to divide it ... That's very useful because it allows you to modify the algorithm as you want. However, to show a basic example, we will use many of the default values and:\n",
    "* Learning rate: eta = 0.1, that it's a typical value for the learning rate and it's the same that we used in other models.\n",
    "* $\\gamma$: gamma = 0 (default), that it's one of the regularization parameters studied in the project.\n",
    "* $\\lambda$: lambda = 1 (default), that it's the other regularization parameter studied.\n",
    "* Build method: tree_method = auto (default), that chooses the optimal method according to the length of the data. We have different methods based on the optimizations commented at the project.\n",
    "\n",
    "Also, we will generate diffent models changing the number of iterations that the model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_Gini</th>\n",
       "      <th>Test_Gini</th>\n",
       "      <th>delta%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGB_1</th>\n",
       "      <td>0.655905</td>\n",
       "      <td>0.626420</td>\n",
       "      <td>-4.495371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_5</th>\n",
       "      <td>0.699824</td>\n",
       "      <td>0.672093</td>\n",
       "      <td>-3.962666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_10</th>\n",
       "      <td>0.712149</td>\n",
       "      <td>0.682763</td>\n",
       "      <td>-4.126433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_20</th>\n",
       "      <td>0.735552</td>\n",
       "      <td>0.709822</td>\n",
       "      <td>-3.498148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_50</th>\n",
       "      <td>0.773221</td>\n",
       "      <td>0.740962</td>\n",
       "      <td>-4.172070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_100</th>\n",
       "      <td>0.799161</td>\n",
       "      <td>0.757099</td>\n",
       "      <td>-5.263301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_200</th>\n",
       "      <td>0.829289</td>\n",
       "      <td>0.768893</td>\n",
       "      <td>-7.282859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_500</th>\n",
       "      <td>0.875273</td>\n",
       "      <td>0.772569</td>\n",
       "      <td>-11.733893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_1000</th>\n",
       "      <td>0.919870</td>\n",
       "      <td>0.770986</td>\n",
       "      <td>-16.185363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Train_Gini  Test_Gini     delta%\n",
       "XGB_1       0.655905   0.626420  -4.495371\n",
       "XGB_5       0.699824   0.672093  -3.962666\n",
       "XGB_10      0.712149   0.682763  -4.126433\n",
       "XGB_20      0.735552   0.709822  -3.498148\n",
       "XGB_50      0.773221   0.740962  -4.172070\n",
       "XGB_100     0.799161   0.757099  -5.263301\n",
       "XGB_200     0.829289   0.768893  -7.282859\n",
       "XGB_500     0.875273   0.772569 -11.733893\n",
       "XGB_1000    0.919870   0.770986 -16.185363"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We generate our XGBoost algorithms:\n",
    "metrics = {}\n",
    "for n_estimators in [1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    model = XGBClassifier(max_depth = 5, eta = 0.1, n_estimators = n_estimators )\n",
    "    model.fit(train[model_columns],train.RainTomorrow);\n",
    "    \n",
    "    train_pred = model.predict_proba(train[model_columns])[:, 1]\n",
    "    test_pred = model.predict_proba(test[model_columns])[:, 1]\n",
    "\n",
    "    metrics['XGB_'+ str(n_estimators)] = {\n",
    "        'Train_Gini': 2*roc_auc_score(train.RainTomorrow, train_pred)-1,\n",
    "        'Test_Gini': 2*roc_auc_score(test.RainTomorrow, test_pred)-1\n",
    "    }\n",
    "\n",
    "metrics_XGB = pd.DataFrame.from_dict(metrics, orient='index',columns=['Train_Gini', 'Test_Gini'])\n",
    "metrics_XGB['delta%'] = 100*(metrics_XGB.Test_Gini - metrics_XGB.Train_Gini) / metrics_XGB.Train_Gini\n",
    "metrics_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM is the abbreviaton of '**Light** **G**radient **B**oosting **M**achine' and is a algorithm developed by Microsoft. It shares his main characteristics with XGBoost, but it builds the trees dividing the nodes that maximize the gain (uses a 'leaf-wise tree growth'). That becomes in assymetric trees with branches more developed than others. In addition, LightGBM applies a set of computational advantages to make it faster.\n",
    "\n",
    "The parameters that we will use are equivalent to the parameters defined in XGBoost. Also, we will compare different models with diferent number of estimators too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_Gini</th>\n",
       "      <th>Test_Gini</th>\n",
       "      <th>delta%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBM_1</th>\n",
       "      <td>0.655849</td>\n",
       "      <td>0.632815</td>\n",
       "      <td>-3.512204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_5</th>\n",
       "      <td>0.704649</td>\n",
       "      <td>0.679644</td>\n",
       "      <td>-3.548661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_10</th>\n",
       "      <td>0.719746</td>\n",
       "      <td>0.695993</td>\n",
       "      <td>-3.300265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_20</th>\n",
       "      <td>0.739624</td>\n",
       "      <td>0.714137</td>\n",
       "      <td>-3.446011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_50</th>\n",
       "      <td>0.776222</td>\n",
       "      <td>0.743210</td>\n",
       "      <td>-4.252954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_100</th>\n",
       "      <td>0.801383</td>\n",
       "      <td>0.758471</td>\n",
       "      <td>-5.354684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_200</th>\n",
       "      <td>0.831010</td>\n",
       "      <td>0.768150</td>\n",
       "      <td>-7.564197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_500</th>\n",
       "      <td>0.881065</td>\n",
       "      <td>0.771816</td>\n",
       "      <td>-12.399557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_1000</th>\n",
       "      <td>0.927716</td>\n",
       "      <td>0.769263</td>\n",
       "      <td>-17.079838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Train_Gini  Test_Gini     delta%\n",
       "LGBM_1       0.655849   0.632815  -3.512204\n",
       "LGBM_5       0.704649   0.679644  -3.548661\n",
       "LGBM_10      0.719746   0.695993  -3.300265\n",
       "LGBM_20      0.739624   0.714137  -3.446011\n",
       "LGBM_50      0.776222   0.743210  -4.252954\n",
       "LGBM_100     0.801383   0.758471  -5.354684\n",
       "LGBM_200     0.831010   0.768150  -7.564197\n",
       "LGBM_500     0.881065   0.771816 -12.399557\n",
       "LGBM_1000    0.927716   0.769263 -17.079838"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We generate our LightGBM algorithms:\n",
    "metrics = {}\n",
    "for n_estimators in [1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    model = LGBMClassifier(max_depth = 5, n_estimators = n_estimators )\n",
    "    model.fit(train[model_columns],train.RainTomorrow);\n",
    "    \n",
    "    train_pred = model.predict_proba(train[model_columns])[:, 1]\n",
    "    test_pred = model.predict_proba(test[model_columns])[:, 1]\n",
    "\n",
    "    metrics['LGBM_'+ str(n_estimators)] = {\n",
    "        'Train_Gini': 2*roc_auc_score(train.RainTomorrow, train_pred)-1,\n",
    "        'Test_Gini': 2*roc_auc_score(test.RainTomorrow, test_pred)-1\n",
    "    }\n",
    "\n",
    "metrics_LGBM = pd.DataFrame.from_dict(metrics, orient='index',columns=['Train_Gini', 'Test_Gini'])\n",
    "metrics_LGBM['delta%'] = 100*(metrics_LGBM.Test_Gini - metrics_LGBM.Train_Gini) / metrics_LGBM.Train_Gini\n",
    "metrics_LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost is the last boosting algorithm studied in the project. His name is the abbreviature of '**Cat**egorical **Boost**ing' and is famous due to the way of deal with categorical variables. These special method allows us to use it directly without encoding the categorical values, so we save preprocessing time.\n",
    "\n",
    "To use it taking advantage of the characteristic commented, we will use a previous and less preprocessed dataset than in the other examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
