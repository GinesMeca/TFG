{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Boosting algortihms\n",
    "\n",
    "This notebook will show applications of boosting algorithms on the dataset [Rain in Australia](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) available in Kaggle. It contains about 10 years of daily weather observations from many locations across Australia.\n",
    "\n",
    "### Index:\n",
    "1. [Packages required](#1.-Packages-required)\n",
    "2. [Loading data](#2.-Loading-data)\n",
    "3. [AdaBoost](#3.-AdaBoost)\n",
    "4. [Gradient Boosting](#4.-Gradient-Boosting)\n",
    "5. [XGBoost](#5.-XGBoost)\n",
    "6. [LightGBM](#6.-LightGBM)\n",
    "7. [CatBoost](#7.-CatBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost\n",
    "!pip install lightgbm\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>Date_month</th>\n",
       "      <th>Date_day</th>\n",
       "      <th>Location_encoded</th>\n",
       "      <th>WindGustDir_encoded</th>\n",
       "      <th>WindDir9am_encoded</th>\n",
       "      <th>WindDir3pm_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0 2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1 2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2 2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3 2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4 2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Temp9am  Temp3pm  RainToday  \\\n",
       "0           W           44.0          W  ...    16.9     21.8          0   \n",
       "1         WNW           44.0        NNW  ...    17.2     24.3          0   \n",
       "2         WSW           46.0          W  ...    21.0     23.2          0   \n",
       "3          NE           24.0         SE  ...    18.1     26.5          0   \n",
       "4           W           41.0        ENE  ...    17.8     29.7          0   \n",
       "\n",
       "   RainTomorrow  Date_month  Date_day  Location_encoded  WindGustDir_encoded  \\\n",
       "0           0.0          12         1                 2                 12.0   \n",
       "1           0.0          12         2                 2                 13.0   \n",
       "2           0.0          12         3                 2                 11.0   \n",
       "3           0.0          12         4                 2                  2.0   \n",
       "4           0.0          12         5                 2                 12.0   \n",
       "\n",
       "   WindDir9am_encoded  WindDir3pm_encoded  \n",
       "0                12.0                13.0  \n",
       "1                15.0                11.0  \n",
       "2                12.0                11.0  \n",
       "3                 6.0                 4.0  \n",
       "4                 3.0                14.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_parquet('../data/04_model_input/master.parquet')\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fix the variables we are interested in and the date to separate data:\n",
    "test_date = '2015-01-01'\n",
    "\n",
    "model_columns = list(set(weather.select_dtypes(include='number').columns) - set(['RainTomorrow']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We separate in train/test data and solve Nan problems:\n",
    "train = weather[weather.Date < test_date].fillna(-1)\n",
    "test = weather[weather.Date >= test_date].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost is a boosting algorithm that reduces the prediction error building (sequentially) trees with only two leave nodes. According to the error from the last estimator, the sample weights are changed and the trees are generated taking into account these different weights.\n",
    "\n",
    "The Python implementation allows you to modify the base estimator, but we won't modify it to use the original one (\"base_estimator = None\" is a decision tree with max_depth = 1). Also, to avoid overfitting, we will use a learning rate $\\nu = 0.1$\n",
    "\n",
    "Then, we will generate different model with different number of boosting iterations to see the evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run_Time</th>\n",
       "      <th>Train_Gini</th>\n",
       "      <th>Test_Gini</th>\n",
       "      <th>delta%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaB_1</th>\n",
       "      <td>0.209618</td>\n",
       "      <td>0.356810</td>\n",
       "      <td>0.324403</td>\n",
       "      <td>-9.082368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_5</th>\n",
       "      <td>0.695875</td>\n",
       "      <td>0.540856</td>\n",
       "      <td>0.518804</td>\n",
       "      <td>-4.077312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_10</th>\n",
       "      <td>1.313694</td>\n",
       "      <td>0.569703</td>\n",
       "      <td>0.540461</td>\n",
       "      <td>-5.132848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_20</th>\n",
       "      <td>2.522580</td>\n",
       "      <td>0.644189</td>\n",
       "      <td>0.607569</td>\n",
       "      <td>-5.684610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_50</th>\n",
       "      <td>6.225233</td>\n",
       "      <td>0.690590</td>\n",
       "      <td>0.660897</td>\n",
       "      <td>-4.299631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_100</th>\n",
       "      <td>12.337142</td>\n",
       "      <td>0.709561</td>\n",
       "      <td>0.690381</td>\n",
       "      <td>-2.703051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_200</th>\n",
       "      <td>24.491446</td>\n",
       "      <td>0.725582</td>\n",
       "      <td>0.711206</td>\n",
       "      <td>-1.981235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_500</th>\n",
       "      <td>61.212872</td>\n",
       "      <td>0.738811</td>\n",
       "      <td>0.723546</td>\n",
       "      <td>-2.066107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaB_1000</th>\n",
       "      <td>130.720991</td>\n",
       "      <td>0.745362</td>\n",
       "      <td>0.727811</td>\n",
       "      <td>-2.354736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Run_Time  Train_Gini  Test_Gini    delta%\n",
       "AdaB_1       0.209618    0.356810   0.324403 -9.082368\n",
       "AdaB_5       0.695875    0.540856   0.518804 -4.077312\n",
       "AdaB_10      1.313694    0.569703   0.540461 -5.132848\n",
       "AdaB_20      2.522580    0.644189   0.607569 -5.684610\n",
       "AdaB_50      6.225233    0.690590   0.660897 -4.299631\n",
       "AdaB_100    12.337142    0.709561   0.690381 -2.703051\n",
       "AdaB_200    24.491446    0.725582   0.711206 -1.981235\n",
       "AdaB_500    61.212872    0.738811   0.723546 -2.066107\n",
       "AdaB_1000  130.720991    0.745362   0.727811 -2.354736"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We generate our AdaBoost algorithms:\n",
    "metrics = {}\n",
    "for n_estimators in [1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    start_time = time.time()\n",
    "    model = AdaBoostClassifier(n_estimators = n_estimators, learning_rate = 0.1)\n",
    "    model.fit(train[model_columns],train.RainTomorrow);\n",
    "    \n",
    "    train_pred = model.predict_proba(train[model_columns])[:, 1]\n",
    "    test_pred = model.predict_proba(test[model_columns])[:, 1]\n",
    "\n",
    "    metrics['AdaB_'+ str(n_estimators)] = {\n",
    "        'Train_Gini': 2*roc_auc_score(train.RainTomorrow, train_pred)-1,\n",
    "        'Test_Gini': 2*roc_auc_score(test.RainTomorrow, test_pred)-1,\n",
    "        'Run_Time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "metrics_AdaB = pd.DataFrame.from_dict(metrics, orient='index',columns=['Run_Time', 'Train_Gini', 'Test_Gini'])\n",
    "metrics_AdaB['delta%'] = 100*(metrics_AdaB.Test_Gini - metrics_AdaB.Train_Gini) / metrics_AdaB.Train_Gini\n",
    "metrics_AdaB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is another boosting algorithm. While AdaBoost modifies the sample weights to build the trees, Gradient Boosting computes the residuals and try to classify them. By this way, the model starts with a big error but the more iterations you make the less error you will be comitting. Also, other difference between AdaBoost and Gradient Boosting is the fact that AdaBoost (originally) builds trees with only two leave nodes and Gradient Boost doesn't have a predetermined number of leaves.\n",
    "\n",
    "About the parameters that we will choose to build our model:\n",
    "* Loss function: loss = log_loss (default), that it's the same that we have been studying at the project.\n",
    "* Learning rate: learning_rate = 0.1 (default), that it's the most common value to avoid overfitting.\n",
    "* Error measure: criterion = mse, that it's the same that we have been studying.\n",
    "\n",
    "Also, we will generate models with different numbers of iterations (n_estimators) to compare them and see clearly the evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run_Time</th>\n",
       "      <th>Train_Gini</th>\n",
       "      <th>Test_Gini</th>\n",
       "      <th>delta%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GB_1</th>\n",
       "      <td>0.390128</td>\n",
       "      <td>0.576891</td>\n",
       "      <td>0.564835</td>\n",
       "      <td>-2.089953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_5</th>\n",
       "      <td>1.294110</td>\n",
       "      <td>0.652841</td>\n",
       "      <td>0.632492</td>\n",
       "      <td>-3.117064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_10</th>\n",
       "      <td>2.448032</td>\n",
       "      <td>0.673436</td>\n",
       "      <td>0.653937</td>\n",
       "      <td>-2.895338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_20</th>\n",
       "      <td>4.840084</td>\n",
       "      <td>0.702074</td>\n",
       "      <td>0.685878</td>\n",
       "      <td>-2.306969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_50</th>\n",
       "      <td>11.738091</td>\n",
       "      <td>0.736219</td>\n",
       "      <td>0.718145</td>\n",
       "      <td>-2.454960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_100</th>\n",
       "      <td>22.058039</td>\n",
       "      <td>0.756702</td>\n",
       "      <td>0.735572</td>\n",
       "      <td>-2.792300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_200</th>\n",
       "      <td>45.079746</td>\n",
       "      <td>0.775952</td>\n",
       "      <td>0.749871</td>\n",
       "      <td>-3.361192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_500</th>\n",
       "      <td>112.713359</td>\n",
       "      <td>0.803554</td>\n",
       "      <td>0.766652</td>\n",
       "      <td>-4.592424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB_1000</th>\n",
       "      <td>225.270067</td>\n",
       "      <td>0.825757</td>\n",
       "      <td>0.771207</td>\n",
       "      <td>-6.606131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Run_Time  Train_Gini  Test_Gini    delta%\n",
       "GB_1       0.390128    0.576891   0.564835 -2.089953\n",
       "GB_5       1.294110    0.652841   0.632492 -3.117064\n",
       "GB_10      2.448032    0.673436   0.653937 -2.895338\n",
       "GB_20      4.840084    0.702074   0.685878 -2.306969\n",
       "GB_50     11.738091    0.736219   0.718145 -2.454960\n",
       "GB_100    22.058039    0.756702   0.735572 -2.792300\n",
       "GB_200    45.079746    0.775952   0.749871 -3.361192\n",
       "GB_500   112.713359    0.803554   0.766652 -4.592424\n",
       "GB_1000  225.270067    0.825757   0.771207 -6.606131"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We generate our Gradient Boosting algorithms:\n",
    "metrics = {}\n",
    "for n_estimators in [1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    start_time = time.time()\n",
    "    model = GradientBoostingClassifier(n_estimators = n_estimators, criterion = 'mse' )\n",
    "    model.fit(train[model_columns],train.RainTomorrow);\n",
    "    \n",
    "    train_pred = model.predict_proba(train[model_columns])[:, 1]\n",
    "    test_pred = model.predict_proba(test[model_columns])[:, 1]\n",
    "\n",
    "    metrics['GB_'+ str(n_estimators)] = {\n",
    "        'Train_Gini': 2*roc_auc_score(train.RainTomorrow, train_pred)-1,\n",
    "        'Test_Gini': 2*roc_auc_score(test.RainTomorrow, test_pred)-1,\n",
    "        'Run_Time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "metrics_GB = pd.DataFrame.from_dict(metrics, orient='index',columns=['Run_Time', 'Train_Gini', 'Test_Gini'])\n",
    "metrics_GB['delta%'] = 100*(metrics_GB.Test_Gini - metrics_GB.Train_Gini) / metrics_GB.Train_Gini\n",
    "metrics_GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is the abbreviation of 'e**X**treme **G**radient **Boost**ing' and is a boosting method based on the last one. XGBoost models apply Gradient Boosting using their own type of trees, which are built taking into account the gradient and the hessian of the Loss Function and some regularization parameters. Also, XGBoost has many computational advantages to do the task faster.\n",
    "\n",
    "XGBoost is a algorithm with a lot of parameters: regularization parameters, maximum depth of the trees, number of iterations, minimum number of subjects in the node to divide it ... That's very useful because it allows you to modify the algorithm as you want. However, to show a basic example, we will use many of the default values and:\n",
    "* Learning rate: eta = 0.1, that it's a typical value for the learning rate and it's the same that we used in other models.\n",
    "* $\\gamma$: gamma = 0 (default), that it's one of the regularization parameters studied in the project.\n",
    "* $\\lambda$: lambda = 1 (default), that it's the other regularization parameter studied.\n",
    "* Build method: tree_method = auto (default), that chooses the optimal method according to the length of the data. We have different methods based on the optimizations commented at the project.\n",
    "\n",
    "Also, we will generate diffent models changing the number of iterations that the model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run_Time</th>\n",
       "      <th>Train_Gini</th>\n",
       "      <th>Test_Gini</th>\n",
       "      <th>delta%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGB_1</th>\n",
       "      <td>0.228267</td>\n",
       "      <td>0.685133</td>\n",
       "      <td>0.645728</td>\n",
       "      <td>-5.751455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_5</th>\n",
       "      <td>0.330508</td>\n",
       "      <td>0.717968</td>\n",
       "      <td>0.684258</td>\n",
       "      <td>-4.695161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_10</th>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.729999</td>\n",
       "      <td>0.694735</td>\n",
       "      <td>-4.830653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_20</th>\n",
       "      <td>0.773927</td>\n",
       "      <td>0.752947</td>\n",
       "      <td>0.717111</td>\n",
       "      <td>-4.759443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_50</th>\n",
       "      <td>1.607960</td>\n",
       "      <td>0.793083</td>\n",
       "      <td>0.747005</td>\n",
       "      <td>-5.809986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_100</th>\n",
       "      <td>3.052148</td>\n",
       "      <td>0.824521</td>\n",
       "      <td>0.762620</td>\n",
       "      <td>-7.507486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_200</th>\n",
       "      <td>6.057560</td>\n",
       "      <td>0.862531</td>\n",
       "      <td>0.770134</td>\n",
       "      <td>-10.712396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_500</th>\n",
       "      <td>17.316261</td>\n",
       "      <td>0.918669</td>\n",
       "      <td>0.771049</td>\n",
       "      <td>-16.068869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB_1000</th>\n",
       "      <td>39.032959</td>\n",
       "      <td>0.966143</td>\n",
       "      <td>0.765852</td>\n",
       "      <td>-20.730979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Run_Time  Train_Gini  Test_Gini     delta%\n",
       "XGB_1      0.228267    0.685133   0.645728  -5.751455\n",
       "XGB_5      0.330508    0.717968   0.684258  -4.695161\n",
       "XGB_10     0.483288    0.729999   0.694735  -4.830653\n",
       "XGB_20     0.773927    0.752947   0.717111  -4.759443\n",
       "XGB_50     1.607960    0.793083   0.747005  -5.809986\n",
       "XGB_100    3.052148    0.824521   0.762620  -7.507486\n",
       "XGB_200    6.057560    0.862531   0.770134 -10.712396\n",
       "XGB_500   17.316261    0.918669   0.771049 -16.068869\n",
       "XGB_1000  39.032959    0.966143   0.765852 -20.730979"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We generate our XGBoost algorithms:\n",
    "metrics = {}\n",
    "for n_estimators in [1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    start_time = time.time()\n",
    "    model = XGBClassifier(eta = 0.1, n_estimators = n_estimators )\n",
    "    model.fit(train[model_columns],train.RainTomorrow);\n",
    "    \n",
    "    train_pred = model.predict_proba(train[model_columns])[:, 1]\n",
    "    test_pred = model.predict_proba(test[model_columns])[:, 1]\n",
    "\n",
    "    metrics['XGB_'+ str(n_estimators)] = {\n",
    "        'Train_Gini': 2*roc_auc_score(train.RainTomorrow, train_pred)-1,\n",
    "        'Test_Gini': 2*roc_auc_score(test.RainTomorrow, test_pred)-1,\n",
    "        'Run_Time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "metrics_XGB = pd.DataFrame.from_dict(metrics, orient='index',columns=['Run_Time', 'Train_Gini', 'Test_Gini'])\n",
    "metrics_XGB['delta%'] = 100*(metrics_XGB.Test_Gini - metrics_XGB.Train_Gini) / metrics_XGB.Train_Gini\n",
    "metrics_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM is the abbreviaton of '**Light** **G**radient **B**oosting **M**achine' and is a algorithm developed by Microsoft. It shares his main characteristics with XGBoost, but it builds the trees dividing the nodes that maximize the gain (uses a 'leaf-wise tree growth'). That becomes in assymetric trees with branches more developed than others. In addition, LightGBM applies a set of computational advantages to make it faster.\n",
    "\n",
    "The parameters that we will use are equivalent to the parameters defined in XGBoost. Also, we will compare different models with diferent number of estimators too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run_Time</th>\n",
       "      <th>Train_Gini</th>\n",
       "      <th>Test_Gini</th>\n",
       "      <th>delta%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LGBM_1</th>\n",
       "      <td>0.238992</td>\n",
       "      <td>0.674456</td>\n",
       "      <td>0.655046</td>\n",
       "      <td>-2.877861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_5</th>\n",
       "      <td>0.268790</td>\n",
       "      <td>0.712575</td>\n",
       "      <td>0.690706</td>\n",
       "      <td>-3.068988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_10</th>\n",
       "      <td>0.293978</td>\n",
       "      <td>0.727493</td>\n",
       "      <td>0.704743</td>\n",
       "      <td>-3.127154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_20</th>\n",
       "      <td>0.380288</td>\n",
       "      <td>0.749691</td>\n",
       "      <td>0.724131</td>\n",
       "      <td>-3.409369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_50</th>\n",
       "      <td>0.522916</td>\n",
       "      <td>0.786070</td>\n",
       "      <td>0.752569</td>\n",
       "      <td>-4.261769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_100</th>\n",
       "      <td>0.759582</td>\n",
       "      <td>0.814562</td>\n",
       "      <td>0.765314</td>\n",
       "      <td>-6.045970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_200</th>\n",
       "      <td>1.101061</td>\n",
       "      <td>0.850011</td>\n",
       "      <td>0.770783</td>\n",
       "      <td>-9.320842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_500</th>\n",
       "      <td>2.165195</td>\n",
       "      <td>0.909996</td>\n",
       "      <td>0.770308</td>\n",
       "      <td>-15.350420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM_1000</th>\n",
       "      <td>4.091079</td>\n",
       "      <td>0.960600</td>\n",
       "      <td>0.767692</td>\n",
       "      <td>-20.082051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Run_Time  Train_Gini  Test_Gini     delta%\n",
       "LGBM_1     0.238992    0.674456   0.655046  -2.877861\n",
       "LGBM_5     0.268790    0.712575   0.690706  -3.068988\n",
       "LGBM_10    0.293978    0.727493   0.704743  -3.127154\n",
       "LGBM_20    0.380288    0.749691   0.724131  -3.409369\n",
       "LGBM_50    0.522916    0.786070   0.752569  -4.261769\n",
       "LGBM_100   0.759582    0.814562   0.765314  -6.045970\n",
       "LGBM_200   1.101061    0.850011   0.770783  -9.320842\n",
       "LGBM_500   2.165195    0.909996   0.770308 -15.350420\n",
       "LGBM_1000  4.091079    0.960600   0.767692 -20.082051"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We generate our LightGBM algorithms:\n",
    "metrics = {}\n",
    "for n_estimators in [1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    start_time = time.time()\n",
    "    model = LGBMClassifier(n_estimators = n_estimators )\n",
    "    model.fit(train[model_columns],train.RainTomorrow);\n",
    "    \n",
    "    train_pred = model.predict_proba(train[model_columns])[:, 1]\n",
    "    test_pred = model.predict_proba(test[model_columns])[:, 1]\n",
    "\n",
    "    metrics['LGBM_'+ str(n_estimators)] = {\n",
    "        'Train_Gini': 2*roc_auc_score(train.RainTomorrow, train_pred)-1,\n",
    "        'Test_Gini': 2*roc_auc_score(test.RainTomorrow, test_pred)-1,\n",
    "        'Run_Time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "metrics_LGBM = pd.DataFrame.from_dict(metrics, orient='index',columns=['Run_Time', 'Train_Gini', 'Test_Gini'])\n",
    "metrics_LGBM['delta%'] = 100*(metrics_LGBM.Test_Gini - metrics_LGBM.Train_Gini) / metrics_LGBM.Train_Gini\n",
    "metrics_LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost is the last boosting algorithm studied in the project. His name is the abbreviature of '**Cat**egorical **Boost**ing' and is famous due to the way of deal with categorical variables. These special method allows us to use it directly without encoding the categorical values, so we save preprocessing time.\n",
    "\n",
    "To use it taking advantage of the characteristic commented, we will use a previous and less preprocessed dataset than in the other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fix the new columns that we will take into account to generate the model:\n",
    "model_columns = list(set(weather.columns) - set(['RainTomorrow', 'Date', 'Location_encoded', 'WindDir3pm_encoded',\n",
    "                                                 'WindDir9am_encoded', 'WindGustDir_encoded']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the encoded variables, we apply the CatBoost algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "Bad value for num_feature[non_default_doc_idx=0,feature_idx=2]=\"W\": Cannot convert 'b'W'' to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.get_float_feature\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._FloatOrNan\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._FloatOrNanFromString\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert 'b'W'' to float",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9bf0171ce4d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_estimators\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRainTomorrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5005\u001b[0m             \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss_function'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5007\u001b[1;33m         self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0m\u001b[0;32m   5008\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5009\u001b[0m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2260\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y may be None only when X is an instance of catboost.Pool or string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2262\u001b[1;33m         train_params = self._prepare_train_params(\n\u001b[0m\u001b[0;32m   2263\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2264\u001b[0m             \u001b[0mpairs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_prepare_train_params\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks)\u001b[0m\n\u001b[0;32m   2146\u001b[0m         \u001b[0membedding_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_feature_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'embedding_features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2148\u001b[1;33m         train_pool = _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs,\n\u001b[0m\u001b[0;32m   2149\u001b[0m                                        \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubgroup_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpairs_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2150\u001b[0m                                        baseline, column_description)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_build_train_pool\u001b[1;34m(X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, column_description)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y has not initialized in fit(): X is not catboost.Pool object, y must be not None in fit().\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m         train_pool = Pool(X, y, cat_features=cat_features, text_features=text_features, embedding_features=embedding_features, pairs=pairs, weight=sample_weight, group_id=group_id,\n\u001b[0m\u001b[0;32m   1431\u001b[0m                           group_weight=group_weight, subgroup_id=subgroup_id, pairs_weight=pairs_weight, baseline=baseline)\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m    788\u001b[0m                     )\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m                 self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n\u001b[0m\u001b[0;32m    791\u001b[0m                            group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n\u001b[0;32m    792\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_init\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\u001b[0m\n\u001b[0;32m   1409\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfeature_tags\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1410\u001b[0m             \u001b[0mfeature_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_transform_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1411\u001b[1;33m         self._init_pool(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n\u001b[0m\u001b[0;32m   1412\u001b[0m                         group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n\u001b[0;32m   1413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._PoolBase._init_features_order_layout_pool\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._set_features_order_data_pd_data_frame\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.create_num_factor_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost.get_float_feature\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: Bad value for num_feature[non_default_doc_idx=0,feature_idx=2]=\"W\": Cannot convert 'b'W'' to float"
     ]
    }
   ],
   "source": [
    "#We generate our CatBoost algorithms:\n",
    "metrics = {}\n",
    "for n_estimators in [1, 5, 10, 20, 50, 100, 200, 500, 1000]:\n",
    "    model = CatBoostClassifier(n_estimators = n_estimators )\n",
    "    model.fit(train[model_columns],train.RainTomorrow);\n",
    "    \n",
    "    train_pred = model.predict_proba(train[model_columns])[:, 1]\n",
    "    test_pred = model.predict_proba(test[model_columns])[:, 1]\n",
    "\n",
    "    metrics['CatB_'+ str(n_estimators)] = {\n",
    "        'Train_Gini': 2*roc_auc_score(train.RainTomorrow, train_pred)-1,\n",
    "        'Test_Gini': 2*roc_auc_score(test.RainTomorrow, test_pred)-1\n",
    "    }\n",
    "\n",
    "metrics_CatB = pd.DataFrame.from_dict(metrics, orient='index',columns=['Train_Gini', 'Test_Gini'])\n",
    "metrics_CatB['delta%'] = 100*(metrics_CatB.Test_Gini - metrics_CatB.Train_Gini) / metrics_CatB.Train_Gini\n",
    "metrics_CatB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
